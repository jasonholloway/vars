
Targets need tobe qualified
by a tag that denotes their scoping



url#auth

will exist as a normal variable, such that blocks taking url will receive it as url; but it will be provided with the specific scoping

so it will be injected with priority given a scoping of auth - in fact, it won't be injected at all if such a scoping is missing

within a scoping, firstly the specific var will be bound, and failing this the generic one


#
# in: aks_uri
# out: auth_uri
@bind auth_uri ${aks_uri}


# in: uri
# out: ip


# in: auth_ip
# out: jwt



# ns: auth
# in: ip
# out: response



so there's always the option of asking for a precisely namespaced var
or of supplying a precisely namespaced var

a non-namespaced input will happily bind to either a namespaced or generic var

the supply willbe direct: auth_uri will supply a var called 'auth_uri'
it's up to the input binding to select the right one

and that input binding can either be particular (which will invoke a scope)
or it will be generic, which will not introduce a scope, might continuate one

not just vars, but blocks themselves are to be qualified by namespaces

----

so:

# n: frisk
# in: auth_ip
# out: blah

will incur an auth scope in trying to find an appropriate input





# n: getIp
# out: ip
a generic block like this can be coopted for the purpose, but it will be modified into:

# n: auth_getIp
# out: auth_ip

this coercion can be done while walking the dependencies and pulling in the blocks
a block can therefore refer to another block, given that the sources of blocks are to be supplied separately:
the coercion will only be done to the block outline



# in: sqlConnString
# out: sqlCmd
#



#
# out: security_sqlConnString
#

# n: security_getUsers
# in: sqlCmd
#

but if inputs are then going to be coerced whenever there's a namespace about


#
# in: security_sqlCmd zuck_sqlCmd
# out: allUsers

------


# out: sqlConnString
case $ns in
  security) @bind sqlConnString "";;
  zuck)     @bind sqlConnString "";;
esac


but by this scheme there will be no shared values: all blocks will be templated into their own peculiar space

which poses problems for initial running, because we will have specialised variables by namespace

userRef, say, will be qualified by namespace, and will not bind to others, even though it is the same user!
we will end up re-resolving userRef, with multiple pickings of the same data

it needs to stand as shared to avoid this ugly fate

so... sometimes blocks should be coerced, and sometimes they should not
the differentiator?

is scoping not transitive then?




# in: security_sqlCmd
makes it quite explicit that the upstream should be namespaced
we have no choice but to coerce, to template here, unless a specific security_sqlCmd is offered
we must take sqlCmd and template it in as security_sqlCmd
sqlCmd however takes as input sqlConnString
which we certainly want to be contextualised by our site of request
it has to be security_sqlConnString

but then there is sortedUri=sorted.com, which should feed into the above



# pin: app=security
# in: sqlCmd
# out: securityUsers
the above pins, but it should only be within, and pinning should automatically create a scope
but what is 'app' here other than a namespace kind of thing?



# pin: customer=boohoo
# in: zuckUri
# out: adminUsers
this is different in that the pinning is not to do with something that can be styled namespace
but is an arbitrarily-chosen pinning of a variable that we would expect to propagate, within the implicit scope of the inputs
of this one block, not commingling with the inputs of others except forthse below


a pinning on a block implies a scope
all blocks below this in the walking of dependencies then are considered with a unique scope on the side
whether met blocks are then templated into the scope then depends on whether they make use of any of the pinnings
only blocks that depend on the customer input, or another transitively determined by the same, will be templated to be unique

------

so we have our target block, and it prescribes a pin
we now walk the tree of upstream blocks with our accumulated context

when we come to an affected block, ie one that consumes said pinning, we must template it out: we copy it with a unique prefix (which still allows us to easily extract the original bid)
then we update downstream references to it recursively

but we must stop at the root of the pinning
the inputs of the root get updated, but the scoping of the prefix does not carry on

---------

and what happens if we have two pinnings about, or even from the same block?

on the same block: it is a single world of pinnings that would form a sac up to the same root, so it would be one context with two pinnings

one nestled within the other: nesting is the word. A new context with prefix would be created for the new situation, but with existing pinnings inherited.

but each pinned var has its own root, which would be rewritten up to depending on the particular pinning encountered.

---------

so...

inputs need to be parsed

this is to be done by /blocks/, in making sense of a section

the pinned vals make up the input expression; these then are rewritten?

so blocks creates an ad-hoc pinning block for each pinned input
and shims the main block

though this then makes the main block available?
which might get used in resolving other bits, unintendedly

----------

instead of being rewritten up top, could the inputs themselves be rewritten?
an input of ip{site=sorted} is canonical enough (with ordering of pins obvs)

what then supplies such an input? 
a block that supplies 'ip' with {site=sorted} on the end
such as getIp{site=sorted}

getIp{site=sorted} would itself resolve to a block with inputs similarly affected

every pinning node gets rewritten to have its own renamed vars
so currently we'd have getIp#1

but this is opaque, an doesn't set us up to share

in finding suppliers of ip{site=sorted},
we first look for an exact match, but fall back to looking for ip, and then sticking {site=sorted} on the end of the block

this injects the block into the pool, and we ask /blocks/ to supply us the usual outline
all inputs will be similarly affected by {site=sorted}

and we get back to deducing...

-----------

but with this we end up with completely separate subtrees
a common jwt is refetched... though, if we cache on actual inputs and actual code, we do get reuse

we don't want to reprompt for the same trivial things (which partial caching would leave us open to). To rely on caching, we'd need to cache _everything_, incurring the cost of hashing

otherwise to share deductions, we need to know which subtrees are affected, and which ones aren't
this requires rewriting, instead of a simple cascading expansion

from each pin point,
we walk the tree, carrying our pins with us, till we get to a node where the pin is needed
from this point up, we rewrite, giving everyhing affected a unique derived name

and each rewritten block is really shimmed: the rewrite therefore introduces loads of shims

the pin rewriting already works of course, so the rewrite could in fact introduce an extra level of shimming to introduce singular pinnings
but to do this, we would meddle with the very rewriting we were performing: we'd have to rewrite in two phases
or have some stack of visitable nodes

-----------

the better alternative is to be sensitive to input expressions in the rewriting
an input expression is a canonical name for a value

so the rewriter recognises input expressions of this form,
and, unless a binding exists specifically for it, it rewrites from there

(as a separate phase of rewriting, to be consolidated soon enough)

------------------------------------------------

if a block _outputs_ a pinned var, then its contribution should be unified with the pinning,
as both are valid contributions; ultimately we can imagine a unifying even of disjunctions (oo-err!)

if a block has a matching input, then we must attach it to the pinning (which means combining it with whatever is existing)

if a block has a matching output, we need to shim it our pinned var (as this is what's needed to match up to downstream renamed inputs)

a site{site=wibble}, if it isn't supplied, should be shortcircuited into the actual value (there is only one possible value apart from the ever-available NIL)
so we never actually inject and bind values; instead we try and whittle down towards an identity vn{vn=val}

what if there's more to the context? ie site{site=wibble+moo=baa}
the second bit is completely irrelevant
though this (implicitly) shortcircuited var still has to be adapted to by a merger if there is a generic supplier of it

in going after weaker upstreams, we should check for weaker variants, with an eye for rewriting them if they exist

-------

we need a clear succession of cases really
developed and tested as a separate component



inputs finessed
outputs decanted via `decant:site:site{site=balls}`



site{site=wibble} is at some point fully worked out, interpreted, consolidated
canonicalized; it should ultimately just be `site`, with its value inherent to it

Or, is the above the canonical form? it makes its value explicit. maybe all other variable expressions tend towards this one.


So... given a set of input outlines, we want certain outputs that we can even test


A ip{site=sorted},url > sortedIp {}
B site > ip                      {site=sorted}
C > site                         {site=sorted}


----------------------------------------------------------

pins are ambient
pins can be explicitly specified via inputs too

so we're back at putting pins in a contextual bag

if we don't keep them as implicit, then the number of pins is hardly going to scale very well
choosing of vars will have unwieldy prompts:

user{cust=BooHoo+env=feature9}>

and this noise will affect all prompts!
if there's an implicated var it will be piled on here

env in the above is sufficiently global to just be implied
though env will often not be needed? it will fal out of consideration as we get to the edges

how about complete free choices to be made?
these instances are affected by the full scope of pinnings

and this baffles the trimming too: all it takes is one open-ended input to pull down all pins
this is at it should be but does pose a problem
the full range of implicated vars needs to be displayed for an informed choice to be made

accepting this weakens the need for the whittling manoeuvre
before we wanted to whittle to avoid drowning the user in info
but this will always be the case
whittling is just one strategy for dealing with the overload of info
the other complementary pincer is to be able to list many more options with the prompt
(needs a printing of all implicated vars above the prompt, in a deemphasising colour)

----------------------------------------

and so, if whittling is put to the side,
what we have are contextualised block executions
each execution has certain pins implicated

links to supplying blocks flow pins
vars can always have certain pins explicitly appended, which add to the pin flow


but then the matching up of vars...
an execution supplying certain vars with certain used pins
can be used to supply another with needs to which it is sufficient

whittling pins then enables reuse of executions
though is this fully needed?
when calling out to external apis - yes, because such calls are costly

we still want these upwellings of values to meet

and to meet they have to have a common base
an execution will be spared if all its inputs are readily supplied by the existing binds





_dog{name=bob+fur=thick}_
can't be supplied by the below
_dog{name=bob}_
because such a dog will have god-knows-what fur

but if you've got a specialized dog, then it can be used for the more generic one

so we need an algebra <cough!> of vars

dog{name=bob} == dog{name=bob+fur=*}

dog{name=bob+fur=*} < dog{name=bob+fur=thick} 

-------

starting with our var target (with its tacked-on pinnings)
we need to then specialize some suppliers for it

if this were with realtime binds, then we'd know what we wanted, but our demands would be too broad because of the baggage we were carrying in the form of top-down pins

two different demands for jwts would be seen as incompatible, and therefore requiring multiple real calls

outlines should carry with them a set of needed vars
the explicit per-block inputs are only the immediate ones


so there'd be an initial wiring-up of blocks, to look ahead and know what needs what

but pinnings would be worked out in realtime?
and blocks specialised in realtime too?
unless sufficient vars were in scope

the gathering of implicated vars up front would let us check for ready-made vars before execution

specialisation would be done as needed then
instead of via up-front rewriting
with vars that need rewriting too

------------------------

and if we're doing at the last moment, there should be chances to ask for last-minute vars from within blocks

and under realtime resolution, vars can be nicely constructed at that point
and checked against the cache of produced values




